<!DOCTYPE html>
<!-- saved from url=(0038)https://ttsdemos.github.io/paper3.html -->
<html lang="en" class="gr__ttsdemos_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>SPCC poster samples</title>
    <link rel="stylesheet" href="./TTS Demos_files/style.css">
</head>
<body data-gr-c-s-loaded="true">
<!-- Part 1: Title, Abstract and Introduction -->
<div>
    <div>
        <h1>
            Samples for "Investigating sequence length in sequence-to-sequence with attention text-to-speech trained on spontaneous speech"
            poster presented at the Speech Processing Crete Courses, 2019
        </h1>
    </div>
    <div style="padding-top:16px;">
        <b>Author:</b>&nbsp;&nbsp;Pilar Oplustil
    </div>

    <div style="padding-top:16px;"><h2>Abstract</h2></div>
    <div style="padding-top:16px;">
        Spontaneous speech datasets have non-constant length audio samples. This data is usually "chunked" to
        obtain suitable length samples for TTS. However, when synthesizing long speech, such as monologues,
        segmentation undermines long range dependencies in speech, such as prosody. Sequence-to-sequence with
        attention (S2SA) models are the latest succeddful architectures in TTS. We are still learning if these
        models can cope with more challenging data, and, specifically, there is no work investigating long
        sequences in S2SA TTS. In this work we have two goals: (A) investigate the effect of sequence length when
        training S2SA and (B) find a method that allows synthesizing long sequences with S2SA
    </div>
</div>

<div>
    <h2 style="padding-top:16px;"><a name="Experiments">Dataset</a></h2>

    <div style="padding-top:16px;">
        The dataset used to train these models is the IBM debater dataset:
        http://www.research.ibm.com/haifa/dept/vst/debating_data.shtml#Debate%20Speech%20Analysis
        These are two audio files from two speakers used at synthesis time from the original dataset.
    </div>

    <table style="padding-top:16px;">
        <tbody><tr>
            <th>AM speaker</th>
            <th>
                <audio src="original_samples/AM_381_monarchy_pro.wav" controls=""></audio>
            </th>
            <th>DJ speaker</th>
            <th>
                <audio src="original_samples/DJ_1_ban-video-games_pro.wav" controls=""></audio>
            </th>
          </tr>
          </tr>
      </tbody></table>

<!-- Part 2: Experiments in Paper -->
<div>
    <h2 style="padding-top:16px;"><a name="Experiments">Experiments</a></h2>

    <h3 style="padding-top:16px;"><a name="Single-reference">Part A: EFFECT OF DIFFERENT SEQUENCE LENGTHS</a></h3>

    <div style="padding-top:16px;">
        We implemented different segmentations methods either based on audio or text to obtain datasets with different
        sequence length distributions and trained S2SA TTS models. Next we show training alignments for the failed models
        described in Table 1. All sentences are different.
    </div>

    <table style="padding-top:16px;">
        <tbody><tr>
            <th></th>
            <th>VAD-1 epoch 500</th>
            <th>VAD-1 epoch 1000</th>
            <th>VAD-3 epoch 500</th>
            <th>VAD-3 epoch 1000</th>
        </tr>
        <tr>
            <th></th>
            <th>
                <img src="./part_A/1_vad1_training_alignment_e500.png" width="300" height="200" alt=""></img>
            </th>
            <th>
                <img src="./part_A/1_vad1_training_alignment_e1000.png" width="300" height="200" alt=""></img>
            </th>
            <th>
                <img src="./part_A/2_vad3_training_alignment_e500.png" width="300" height="200" alt=""></img>
            </th>
            <th>
                <img src="./part_A/2_vad3_training_alignment_e1000.png" width="300" height="200" alt=""></img>
            </th>
        </tr>
    </tbody></table>


    </tbody></table>
    <table style="padding-top:16px;">
        <tbody><tr>
            <th></th>
            <th>Binary classifier epoch 500</th>
            <th>Binary classifier epoch 1000</th>
            <th>Baseline epoch 500</th>
            <th>Baseline classifier epoch 1000</th>
        </tr>
        <tr>
            <th></th>
            <th>
                <img src="./part_A/4_classifier_training_alignment_e500.png" width="300" height="200" alt=""></img>
            </th>
            <th>
                <img src="./part_A/4_classifier_training_alignment_e1000.png" width="300" height="200" alt=""></img>
            </th>
            <th>
                <img src="./part_A/5_baseline_training_alignment_e500.png" width="300" height="200" alt=""></img>
            </th>
            <th>
                <img src="./part_A/5_baseline_training_alignment_e1000.png" width="300" height="200" alt=""></img>
            </th>
        </tr>
    </tbody></table>

    <div style="padding-top:16px;">
        The training alignments are very interesting. We can see for VAD-1 and VAD-3 that attention changed from epoch
        500 to 1000, specially at the beginning, showing that an alignment was starting to form. This might mean that
        if trained for longer, these models might eventually learn a proper alignment. On the other hand, the binary
        classifier seemed to have learnt a good alignment at the start but it fades out up to the end, while the
        baseline shows an alignment for a very short sentence which seems quite strong. It seems then that the
        learning of the attention is not indifferent to the chunking method used and to how clean it is.

    </div>


    <div style="padding-top:16px;">
        These are samples for the failed models in Table 2. We can see that for the classifier and baseline models
        some words can be recognized and at some portions a bit of structure, while the VAD models are clearly
        generating random speech-like sounds. All these samples were synthesized using the code for speaker "DJ",
        which only the baseline models seems to have learnt speaker characteristics. All sentences are different.

    </div>

    <table style="padding-top:16px;">
        <tbody><tr>
            <th>VAD-1</th>
            <th>
                <audio src="part_A/1_vad1_failed_synthesis.wav" controls=""></audio>
            </th>
            <th>VAD-3</th>
            <th>
                <audio src="part_A/2_vad3_failed_synthesis.wav" controls=""></audio>
            </th>
            <th>Binary classifier</th>
            <th>
                <audio src="part_A/4_classifier_failed_synthesis.wav" controls=""></audio>
            </th>
            <th>Baseline</th>
            <th>
                <audio src="part_A/5_baseline_failed_synthesis.wav" controls=""></audio>
            </th>
        </tr>
        </tr>
    </tbody></table>

    <div>
        <h2 style="padding-top:16px;"><a name="Experiments">Experiments</a></h2>

        <h3 style="padding-top:16px;"><a name="Single-reference">Part B: SYNTHESIZING LONG SEQUENCES</a></h3>

        <div style="padding-top:16px;">
            These are training alignments for the "scheduled" model described in Table 3. The alignment of first
            phase is not shown as it's almost unnoticeable, and all sentences are different.
        </div>

        <table style="padding-top:16px;">
            <tbody><tr>
                <th></th>
                <th>Phase 1</th>
                <th>Phase 2</th>
                <th>Phase 3</th>
            </tr>
            <tr>
                <th></th>
                <th>
                    <img src="./part_B/combined_model_alignments/combined_train_align_end_phase2.png" width="400" height="300" alt=""></img>
                </th>
                <th>
                    <img src="./part_B/combined_model_alignments/combined_train_align_end_phase3.png" width="400" height="300" alt=""></img>
                </th>
                <th>
                    <img src="./part_B/combined_model_alignments/combined_train_align_end_phase4.png" width="400" height="300" alt=""></img>
                </th>
            </tr>
        </tbody></table>




        <div style="padding-top:16px;">
            We synthesized a very long paragraph with each system, this is the text:
        </div>


        <div style="padding-top:16px;">
            "think that western liberal democracies are more specifically obligated to developing countries and their citizens because they often are partially responsible for their circumstances. Like Like america literally created isis when they shipped off weapons to like rebel groups in iran, and the britons the brits literally destroyed african independence by redrawing the borders and just kind of colonizing all over the place. Same Same with india. When When states cause the problems of others, we think they have an obligation to assist with the consequences given that they caused them. We We think that taking in those affected by their actions is an effective way to do so, and like possibly one of the best ways in so far as you are saving lives without like with minimum expenditure on yourself. Like Like we think a really important thing in this round is that states aren't actually harmed by taking in more immigrants. Like Like sure, there are arguments about why it might be difficult for your welfare system but all of these things even out over time. Like Like we had many studies that have shown that immigrants, when integrated, actually become very productive members of society, and give birth to children that become very productive members of society. Our Our second of clash has to do with the arbitrary nature of citizenship. Because Because even if you think that obligations occur first to your citizens, we don't think that applies if the way by which people can be citizens is arbitrary and there is no like fixed standard by which we measure. So So both"
        </div>

        <table style="padding-top:16px;">
            <tbody><tr>
                <th>Grammar model</th>
                <th>
                    <audio src="part_B/paragraph_synthesis/paragraph_grammar_synth_AM_spk.wav" controls=""></audio>
                </th>
                <th>Scheduled model</th>
                <th>
                    <audio src="part_B/paragraph_synthesis/paragraph_combined_synth_AM_spk.wav" controls=""></audio>
                </th>
            </tr>
        </tbody></table>

        <div style="padding-top:16px;">
            Next we highlight some interesting behaviour observed when comparing the two paragraphs.
        </div>


        <div style="padding-top:16px;">
            1. The scheduled model "respects" punctuation unlike the grammar based model, in the sentence:
            "colonizing all over the place. Same Same with india. When When states cause the problems of others"
        </div>

        <table style="padding-top:16px;">
            <tbody><tr>
                <th>Grammar based</th>
                <th>
                    <audio src="part_B/paragraph_synthesis/Analysis/2_nltk_india_sentence.wav" controls=""></audio>
                </th>
                <th>Scheduled based</th>
                <th>
                    <audio src="part_B/paragraph_synthesis/Analysis/2_combined_india_sentence.wav" controls=""></audio>
                </th>
            </tr>
        </tbody></table>

        <div style="padding-top:16px;">
            2. Both models add perceivable boundaries at places where there is not punctuation in the text, in similar
            places
        </div>

        <div style="padding-top:16px;">
            "best ways so far as you are saving lives without like with minimum expenditure on yourself. Like"
        </div>

        <table style="padding-top:16px;">
            <tbody><tr>
                <th>Grammar based</th>
                <th>
                    <audio src="part_B/paragraph_synthesis/Analysis/4_nltk_so_far_as_you_are_saving_lives_without_like_with_minimum_expenditure_on_yourself.wav" controls=""></audio>
                </th>
                <th>Scheduled based</th>
                <th>
                    <audio src="part_B/paragraph_synthesis/Analysis/7_combined_you_are_saving_lives_without_like_with_minimum_expenditure_on_yourself.wav" controls=""></audio>
                </th>
            </tr>
        </tbody></table>

        <div style="padding-top:16px;">
            3. Both models keep emphasis, and at similar locations
        </div>

        <div style="padding-top:16px;">
            "Because Because even if you think that obligations occur first to your citizens"
        </div>

        <table style="padding-top:16px;">
            <tbody><tr>
                <th>Grammar based</th>
                <th>
                    <audio src="part_B/paragraph_synthesis/Analysis/5_nltk_Because_Because_even_if_you_think_that_obligations_occur_first_to_your_citizens.wav" controls=""></audio>
                </th>
                <th>Scheduled based</th>
                <th>
                    <audio src="part_B/paragraph_synthesis/Analysis/5_combined_Because_Because_even_if_you_think_that_obligations_occur_first_to_your_citizens.wav" controls=""></audio>
                </th>
            </tr>
        </tbody></table>


</body></html>
